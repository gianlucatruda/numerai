{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1664736759316,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"e9usqENyI-hD"},"outputs":[],"source":["UPLOAD_PREDS = \"True\" #@param ['False', 'True']\n","SAVE_PREDS = \"True\" #@param ['False', 'True']\n","MODEL_NAME = '<model_name_here>' #@param {type:\"string\"}\n","MODEL_ID = '<model_id_here>' #@param {type:\"string\"}"]},{"cell_type":"markdown","metadata":{"id":"r5zRASAHJehY"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"pL2LDrw1Dwie"},"source":["## Connecting Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20053,"status":"ok","timestamp":1664736779365,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"cPSBYVpJD1-j","outputId":"bd0910e3-998b-4bd9-c1e4-a9369510742e"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1664736779366,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"NIyKAA9GETgx"},"outputs":[],"source":["from pathlib import Path\n","DIR = Path('gdrive/MyDrive/numerai')\n","DATADIR = DIR / 'data'\n","SRCDIR = DIR / 'src'\n","RESULTDIR = DIR / 'results'"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1720,"status":"ok","timestamp":1664736781080,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"lMdQ-fIzUthj"},"outputs":[],"source":["# Copy .env from numerai folder to root dir\n","!cp gdrive/MyDrive/Data/numerai/.env .env"]},{"cell_type":"markdown","metadata":{"id":"ma62H4nY5DOt"},"source":["## Installing and Importing Dependencies\n","First, we install and import the necessary packages. This cell is currently set *not* to print any output; if you run into any issues and need to check for error messages, comment out the `%%capture` line"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":22030,"status":"ok","timestamp":1664736803103,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"6fitBsr_ty8_"},"outputs":[],"source":["%%capture\n","# install\n","!pip uninstall --no-input pandas\n","!pip install --upgrade python-dotenv fastai numerapi\n","!pip install ipython-autotime\n","!pip install torchmetrics\n","\n","# import dependencies\n","import gc\n","import os\n","import csv\n","from dotenv import load_dotenv, find_dotenv\n","from getpass import getpass\n","import numerapi\n","from fastai.tabular.all import *\n","from pathlib import Path\n","from scipy.stats import spearmanr\n","import sklearn.linear_model\n","\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","import torch"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1664736803104,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"MidvwlsQDl8D","outputId":"f81dc65d-2629-43ea-9b95-7508a7185717"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU count: 8\n","time: 265 Âµs (started: 2022-10-02 18:53:22 +00:00)\n"]}],"source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","# Set sensible defaults\n","sns.set()\n","sns.set_style(\"ticks\")\n","sns.set_context('paper')\n","\n","# Get number of CPUs\n","import multiprocessing\n","CPUs = multiprocessing.cpu_count()\n","print(f\"CPU count: {CPUs}\")\n","\n","%load_ext autotime"]},{"cell_type":"markdown","metadata":{"id":"T8k1mucsueRZ"},"source":["## Setting up numerapi\n","We will use the [numerapi](https://github.com/uuazed/numerapi) package to access the data and make submissions. For this to work, numerapi needs to use your API keys (which can be obtained [here](https://numer.ai/submit)). We will set up two main ways of passing these API keys to a numerapi instance:\n","1. Read a `.env` file using the `python-dotenv` package. This will require you to upload a `.env` file (which contains your secret key and should *not* be kept under version control). Using this method means you will not have to directly enter your keys each time you use this notebook, though you will need to re-upload the `.env` file.\n","2. Manually entering the API keys -- if you don't have access to, or don't want to mess with, your `.env` file.\n","\n","If you have a `.env` file, upload it to the default working directory, `content`, now. In either case, run the cell below to set up the numerapi instance. See [Appendix A](#app_a) for instructions on generating and downloading a .env file."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664736803104,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"1Z1CS2Uwv79C","outputId":"e047f121-018e-4a9c-d847-92e9b1a6c7df"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded Numerai Public Key into Global Environment!\n","Loaded Numerai Secret Key into Global Environment!\n","time: 3.72 ms (started: 2022-10-02 18:53:22 +00:00)\n"]}],"source":["# Load the numerapi credentials from .env or prompt for them if not available\n","def credential():\n","    dotenv_path = find_dotenv()\n","    load_dotenv(dotenv_path)\n","\n","    if os.getenv(\"NUMERAI_PUBLIC_KEY\"):\n","        print(\"Loaded Numerai Public Key into Global Environment!\")\n","    else:\n","        os.environ[\"NUMERAI_PUBLIC_KEY\"] = getpass(\"Please enter your Numerai Public Key. You can find your key here: https://numer.ai/submit -> \")\n","\n","    if os.getenv(\"NUMERAI_SECRET_KEY\"):\n","        print(\"Loaded Numerai Secret Key into Global Environment!\")\n","    else:\n","        os.environ[\"NUMERAI_SECRET_KEY\"] = getpass(\"Please enter your Numerai Secret Key. You can find your key here: https://numer.ai/submit -> \")\n","\n","credential()\n","public_key = os.environ.get(\"NUMERAI_PUBLIC_KEY\")\n","secret_key = os.environ.get(\"NUMERAI_SECRET_KEY\")\n","napi = numerapi.NumerAPI(verbosity=\"info\", public_id=public_key, secret_key=secret_key)"]},{"cell_type":"markdown","metadata":{"id":"w1LhPvUS7RLk"},"source":["You can read up on the functionality of numerapi [here](https://github.com/uuazed/numerapi). You can use it to download the competition data, view other numerai users' public profiles, check submission status, manage your stake, and much more. In this case, we'll only be using it to download competition data and submit predictions.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AWg8UlaTGMWj"},"source":["# Data preparation\n"]},{"cell_type":"markdown","metadata":{"id":"7bVmml-fGQvz"},"source":["## Downloading Competition Data\n","In a more structured project, you'll probably want to keep the data in a seprate directory from your scripts etc. You could also link google colab to your google drive and store the data there in order to avoid needing to download and process the data every time. In this case, however, we'll keep everything in `./content`, and download the data fresh each time."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1664736803105,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"Rs_4xI8pGWj5","outputId":"afedcf5f-2add-4e71-a922-f7a5b6258897"},"outputs":[{"name":"stdout","output_type":"stream","text":["no new round within the last 24 hours\n","time: 270 ms (started: 2022-10-02 18:53:22 +00:00)\n"]}],"source":["# check if a new round has started\n","if napi.check_new_round():\n","    print(\"new round has started within the last 24hours!\")\n","else:\n","    print(\"no new round within the last 24 hours\")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1307,"status":"ok","timestamp":1664736804404,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"VcCNcVMv7y1B","outputId":"2ba210cd-e654-418c-8d9c-d84fd262811f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current round already downloaded\n","time: 659 ms (started: 2022-10-02 18:53:22 +00:00)\n"]}],"source":["# Download the current dataset unless it's already there\n","if not os.path.exists(f'{DATADIR}/numerai_dataset_{napi.get_current_round()}'):\n","    napi.download_current_dataset(dest_path=DATADIR, unzip=True)\n","else:\n","    print(\"Current round already downloaded\")"]},{"cell_type":"markdown","metadata":{"id":"hm7lo6fe8Qsy"},"source":["## Reading the data into memory\n","\n","If you look at the files we downloaded above, you'll see a `numerai_tournament_data.csv` file and a `numerai_training_data.csv` file. The \"tournament\" file contains many rows with targets which we can use for validation, so let's extract those and combine them with our training set. "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94093,"status":"ok","timestamp":1664736898493,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"bmymMiRn_t3s","outputId":"027a5a83-126b-4210-e8f4-6c09e370babc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Round number 336\n","Detected 314 columns\n","Reading tournament data...\n","time: 1min 33s (started: 2022-10-02 18:53:23 +00:00)\n"]}],"source":["# Get the current round\n","ROUND_NUM = napi.get_current_round()\n","# ROUND_NUM = 260\n","print(f\"Round number {ROUND_NUM}\")\n","\n","train_file = Path(f'{DATADIR}/numerai_dataset_{ROUND_NUM}/numerai_training_data.csv')\n","tourn_file = Path(f'{DATADIR}/numerai_dataset_{ROUND_NUM}/numerai_tournament_data.csv')\n","\n","# Load training column names only, so we can specify data types\n","with open(train_file, 'r') as f:\n","    column_names = next(csv.reader(f))\n","    print(f\"Detected {len(column_names)} columns\")\n","\n","# Specify the datatypes in memory-efficient way\n","DTYPES = {c: 'float16' for c in column_names if c.startswith(('feature', 'target'))}\n","\n","# tournament data contains features only (for the test rows)\n","print('Reading tournament data...')\n","df_tourn = pd.read_csv(tourn_file, dtype=DTYPES, engine='c').set_index(\"id\")\n","df = df_tourn"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1664736898494,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"6k3Hx2PmIM6k","outputId":"852c5594-9335-4e58-8d10-d14b3222ed8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 1.71 ms (started: 2022-10-02 18:54:57 +00:00)\n"]}],"source":["# Get the names of the features\n","RAW_FEAT_COLS = [f for f in df.columns if \"feature\" in f]\n","FEAT_COLS = RAW_FEAT_COLS # identical prior to feature engineering\n","\n","# Get the groups of the features\n","FEAT_GROUPS = {\n","    g: [c for c in RAW_FEAT_COLS if c.startswith(f\"feature_{g}\")]\n","    for g in [\"intelligence\", \"wisdom\", \"charisma\", \"dexterity\", \"strength\", \"constitution\"]\n","}"]},{"cell_type":"markdown","metadata":{"id":"WeYfQcry6svF"},"source":["# Evaluation Metrics\n","\n","In this section, we will define two key evaluation metrics used to assess the performance of models before submitting to the tournament. These metrics are:\n","- Average Spearman Correlation per era: The sum of each era's Spearman correlation divided by the number of eras.\n","- Sharpe Ratio: The average correlation per era divided by the standard deviation of the correlations per era.\n","\n","Both are defined in reasonable detail [here](https://wandb.ai/carlolepelaars/numerai_tutorial/reports/How-to-get-Started-With-Numerai--VmlldzoxODU0NTQ). The methods defined below are modified versions of the methods described in that post."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1664736898495,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"DtFniCX98z6i","outputId":"2c7d4c12-f0aa-41c1-dc56-b7b8de9e230c"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 6.03 ms (started: 2022-10-02 18:54:57 +00:00)\n"]}],"source":["def score_corr(df: pd.DataFrame) -> np.float32:\n","    \"\"\"\n","    Calculate the correlation by using grouped per-era data\n","    :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and \"prediction\"\n","    :return: The average per-era correlations.\n","    \"\"\"\n","    def _score(sub_df: pd.DataFrame) -> np.float32:\n","        \"\"\" Calculate Spearman correlation for Pandas' apply method \"\"\"\n","        return spearmanr(sub_df[\"target\"],  sub_df[\"prediction\"])[0]\n","    corrs = df.groupby(\"era\").apply(_score)\n","    return corrs.mean()\n","\n","def score_spear(y_true, y_pred, axis=0):\n","    \"\"\"Calculate Spearman correlation\"\"\"\n","    return spearmanr(y_true, y_pred, axis=axis)[0]\n","\n","def score_sharpe(df: pd.DataFrame) -> np.float32:\n","    \"\"\"\n","    Calculate the Sharpe ratio by using grouped per-era data\n","    :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and \"prediction\"\n","    :return: The Sharpe ratio for your predictions.\n","    \"\"\"\n","    def _score(sub_df: pd.DataFrame) -> np.float32:\n","        \"\"\" Calculate Spearman correlation for Pandas' apply method \"\"\"\n","        return spearmanr(sub_df[\"target\"],  sub_df[\"prediction\"])[0]\n","    corrs = df.groupby(\"era\").apply(_score)\n","    return corrs.mean() / corrs.std()\n","\n","def feature_exposures(df: pd.DataFrame, preds: pd.Series, feat_cols=None):\n","    \"\"\" Calculate feature exposure of a model's predictions.\n","        https://forum.numer.ai/t/model-diagnostics-feature-exposure/899\n","    \"\"\"\n","    if feat_cols is None:\n","        feat_cols = RAW_FEAT_COLS\n","        # Use the raw feature columns by default\n","\n","    exposures = []\n","    for f in feat_cols:\n","        fe = spearmanr(preds, df[f])[0]\n","        exposures.append(fe)\n","    return np.array(exposures)\n","\n","def scores(df: pd.DataFrame, verbose=False, feat_cols=None) -> (np.float32, np.float32):\n","    \"\"\" Score models across a variety of metrics. \"\"\"\n","    if feat_cols is None:\n","        feat_cols = RAW_FEAT_COLS\n","    val_sharpe = score_sharpe(df)\n","    val_corr = score_corr(df)\n","    fe = feature_exposures(df, df['prediction'], feat_cols=feat_cols)\n","    max_fe = np.max(fe)\n","    rms_fe = np.sqrt(np.mean(np.square(fe)))\n","\n","    if verbose:\n","        print(f'Spearman:\\t{val_corr:.4f}')\n","        print(f'Sharpe:\\t\\t{val_sharpe:.4f}')\n","        print(f'Max exposure:\\t{max_fe:.4f}')\n","        print(f'RMS exposure:\\t{rms_fe:.4f}')\n","\n","    return val_corr, val_sharpe, max_fe, rms_fe\n","\n","def visualise_feat_exposure(df: pd.DataFrame, model, feat_cols: list, fe_cols: list):\n","    \"\"\" Visualises mean and max feature exposure for a trained model over eras\n","    \"\"\"\n","    _df = df.copy()\n","    _df['prediction'] = model.predict(_df[feat_cols])\n","\n","    maxes, means, eras = [], [], _df.era.unique()\n","    for era in tqdm(eras):\n","        era_df = _df[_df.era == era]\n","        era_fe = feature_exposures(era_df, era_df['prediction'], feat_cols=fe_cols)\n","        maxes.append(era_fe.max())\n","        means.append(np.sqrt(np.mean(np.square(era_fe))))\n","\n","    plt.plot(eras, means, marker='.', label='Mean FE')\n","    plt.plot(eras, maxes, marker='.', label='Max FE')\n","    plt.legend()\n","    plt.xlabel('Era')\n","    plt.ylabel('Exposure')\n","    plt.title(f'Exposure of {len(fe_cols)} features by era')\n"]},{"cell_type":"markdown","metadata":{"id":"8BSr1BMR5Ilh"},"source":["# Modeling the Data"]},{"cell_type":"markdown","metadata":{"id":"ZF1m77ij33wT"},"source":["### Autoencoder model"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12767,"status":"ok","timestamp":1664736911252,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"67clPNkEueJJ","outputId":"e339f170-061b-4bc5-f9bf-8357be4a858e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda:0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv1d-1               [-1, 4, 153]              24\n","         LeakyReLU-2               [-1, 4, 153]               0\n","            Conv1d-3                [-1, 8, 75]             168\n","       BatchNorm1d-4                [-1, 8, 75]              16\n","         LeakyReLU-5                [-1, 8, 75]               0\n","            Conv1d-6               [-1, 16, 24]             656\n","       BatchNorm1d-7               [-1, 16, 24]              32\n","         LeakyReLU-8               [-1, 16, 24]               0\n","            Conv1d-9                [-1, 32, 7]           2,592\n","      BatchNorm1d-10                [-1, 32, 7]              64\n","        LeakyReLU-11                [-1, 32, 7]               0\n","           Conv1d-12                [-1, 64, 5]           6,208\n","             ReLU-13                [-1, 64, 5]               0\n","  ConvTranspose1d-14                [-1, 32, 9]          10,272\n","        LeakyReLU-15                [-1, 32, 9]               0\n","  ConvTranspose1d-16               [-1, 16, 37]           2,576\n","      BatchNorm1d-17               [-1, 16, 37]              32\n","        LeakyReLU-18               [-1, 16, 37]               0\n","  ConvTranspose1d-19               [-1, 8, 151]             904\n","      BatchNorm1d-20               [-1, 8, 151]              16\n","        LeakyReLU-21               [-1, 8, 151]               0\n","  ConvTranspose1d-22               [-1, 1, 310]              81\n","          Sigmoid-23               [-1, 1, 310]               0\n","================================================================\n","Total params: 23,641\n","Trainable params: 23,641\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.09\n","Params size (MB): 0.09\n","Estimated Total Size (MB): 0.18\n","----------------------------------------------------------------\n","time: 13.6 s (started: 2022-10-02 18:54:57 +00:00)\n"]}],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","from torch.cuda.amp import autocast\n","from torchsummary import summary\n","from sklearn.model_selection import train_test_split\n","\n","# Enable CUDA GPU support, if available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print('Device:', device)\n","\n","# Write my own dataset object\n","class NumeraiDataset(Dataset):\n","\n","    def __init__(self, df: pd.DataFrame):\n","        self.df = df[FEAT_COLS].copy()\n","        self.df = self.df.reset_index(drop=True)\n","        self.target = df['target'].values\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        X = torch.as_tensor(self.df.iloc[idx, :].values, dtype=torch.float32)\n","        y = torch.as_tensor(self.target[idx], dtype=torch.float32)\n","\n","        return (X, y)\n","\n","\n","def train_net(net, trainloader, valloader, optimiser, enc_criterion, pred_criterion, epochs=1, checkpoints=None):\n","    \"\"\" Train the network for the specified number of epochs, tracking train\n","    and validation losses.\n","    \"\"\"\n","\n","    if checkpoints is not None:\n","        checkpoints = Path(checkpoints)\n","        if not os.path.exists(checkpoints):\n","            os.makedirs(str(checkpoints))\n","\n","    # Create lists for tracking loss and accuracy\n","    epoch_losses_enc, epoch_val_losses_enc, epoch_losses_pred, epoch_val_losses_pred = [], [], [], []\n","\n","    try:\n","        # loop over the dataset multiple times\n","        for epoch in tqdm(range(epochs), desc='Train'):\n","            running_loss_enc, running_loss_pred = 0.0, 0.0\n","\n","            # Get a minibatch of data\n","            for i, (inputs, labels) in enumerate(trainloader, 0):\n","\n","                # Move data to GPU (if available)\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                # Reset gradients\n","                optimiser.zero_grad()\n","\n","                # Forward pass (of autoencoder)\n","                outputs = net(inputs)\n","\n","                # Calculate loss of autoencoder\n","                enc_loss = enc_criterion(outputs, inputs)\n","                # Calculate the loss of predictor\n","                preds = net.predict(inputs)\n","                pred_loss = pred_criterion(preds, labels)\n","\n","                # Update and log training losses\n","                running_loss_enc += enc_loss.item()\n","                running_loss_pred += pred_loss.item()\n","\n","                # Backpropagation and weight updating\n","                enc_loss.backward()\n","                pred_loss.backward()\n","                optimiser.step()\n","\n","            # Calculate avg loss over epoch\n","            epoch_losses_enc.append(running_loss_enc / len(trainloader.dataset))\n","            epoch_losses_pred.append(running_loss_pred / len(trainloader.dataset))\n","\n","            print(f\"Training loss | Autoencoder: {epoch_losses_enc[-1]:.4f}\\tPredictor: {epoch_losses_pred[-1]:.4f}\")\n","\n","            # Calculate validation loss over epoch\n","            val_loss_enc, val_loss_pred = 0.0, 0.0\n","            val_steps = 0\n","            for i, data in enumerate(valloader, 0):\n","                with torch.no_grad():\n","                    inputs, labels = data\n","                    # Move data to GPU (if available)\n","                    inputs, labels = inputs.to(device), labels.to(device)\n","                    outputs = net(inputs)\n","                    enc_loss = enc_criterion(outputs, inputs)\n","                    val_loss_enc += enc_loss.item()\n","                    preds = net.predict(inputs)\n","                    pred_loss = pred_criterion(preds, labels)\n","                    val_loss_pred += pred_loss.item()\n","                    val_steps += 1\n","            epoch_val_losses_enc.append(val_loss_enc / len(valloader.dataset))\n","            epoch_val_losses_pred.append(val_loss_pred / len(valloader.dataset))\n","            print(f\"Validation loss | Autoencoder: {epoch_val_losses_enc[-1]:.4f}\\tPredictor: {epoch_val_losses_pred[-1]:.4f}\")\n","\n","            # Checkpointing model states\n","            if checkpoints is not None:\n","                if epoch_val_losses_enc[-1] <= np.min(epoch_val_losses_enc) and epoch_val_losses_pred[-1] <= np.min(epoch_val_losses_pred):\n","                    print(\"Best model. Checkpointing...\")\n","                    torch.save({\n","                        'epoch': epoch,\n","                        'model_state_dict': net.state_dict(),\n","                        'optimizer_state_dict': optimiser.state_dict(),\n","                        'val_losses_pred': epoch_val_losses_pred,\n","                        'val_losses_enc': epoch_val_losses_enc,\n","                    }, checkpoints/'best_checkpoint.tar')\n","\n","\n","        print('\\nTraining complete')\n","\n","    except KeyboardInterrupt as e:\n","        print(f'\\nKeyboard interrupt! Stopping training after {epoch} of {epochs}...')\n","\n","    return epoch_losses_enc, epoch_val_losses_enc, epoch_losses_pred, epoch_val_losses_pred\n","\n","\n","class Autoencoder(nn.Module):\n","    # Based on https://github.com/astorfi/differentially-private-cgan/blob/master/UCI/autoencoder.py\n","\n","    def __init__(self):\n","        super(Autoencoder, self).__init__()\n","        n_channels_base = 4\n","\n","        self.encoder = nn.Sequential(\n","            nn.Conv1d(in_channels=1, out_channels=n_channels_base, kernel_size=5, stride=2, padding=0, dilation=1,\n","                      groups=1, bias=True, padding_mode='zeros'),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv1d(in_channels=n_channels_base, out_channels=2 * n_channels_base, kernel_size=5, stride=2, padding=0,\n","                      dilation=1,\n","                      groups=1, bias=True, padding_mode='zeros'),\n","            nn.BatchNorm1d(2 * n_channels_base),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv1d(in_channels=2 * n_channels_base, out_channels=4 * n_channels_base, kernel_size=5, stride=3,\n","                      padding=0, dilation=1,\n","                      groups=1, bias=True, padding_mode='zeros'),\n","            nn.BatchNorm1d(4 * n_channels_base),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv1d(in_channels=4 * n_channels_base, out_channels=8 * n_channels_base, kernel_size=5, stride=3,\n","                      padding=0, dilation=1,\n","                      groups=1, bias=True, padding_mode='zeros'),\n","            nn.BatchNorm1d(8 * n_channels_base),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv1d(in_channels=8 * n_channels_base, out_channels=16 * n_channels_base, kernel_size=3, stride=1,\n","                      padding=0, dilation=1,\n","                      groups=1, bias=True, padding_mode='zeros'),\n","            nn.ReLU(),\n","        )\n","\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose1d(in_channels=16 * n_channels_base, out_channels=8 * n_channels_base, kernel_size=5,\n","                               stride=1, padding=0, dilation=1,\n","                               groups=1, bias=True, padding_mode='zeros'),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose1d(in_channels=8 * n_channels_base, out_channels=4 * n_channels_base, kernel_size=5,\n","                               stride=4, padding=0,\n","                               dilation=1,\n","                               groups=1, bias=True, padding_mode='zeros'),\n","            nn.BatchNorm1d(4 * n_channels_base),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose1d(in_channels=4 * n_channels_base, out_channels=2 * n_channels_base, kernel_size=7,\n","                               stride=4,\n","                               padding=0, dilation=1,\n","                               groups=1, bias=True, padding_mode='zeros'),\n","            nn.BatchNorm1d(2 * n_channels_base),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.ConvTranspose1d(in_channels=2 * n_channels_base, out_channels=1, kernel_size=10, stride=2,\n","                               padding=0, dilation=1,\n","                               groups=1, bias=True, padding_mode='zeros'),\n","            nn.Sigmoid(),\n","        )\n","\n","        self.fc = nn.Sequential(\n","           nn.Linear(in_features=320+310, out_features=256),\n","           nn.ReLU(),\n","           nn.Linear(in_features=256, out_features=64),\n","           nn.ReLU(),\n","           nn.Linear(in_features=64, out_features=1),\n","           nn.Sigmoid(),\n","        )\n","\n","\n","    def forward(self, x):\n","        x = self.encoder(x.view(-1, 1, x.shape[1]))\n","        x = self.decoder(x)\n","        return torch.squeeze(x, dim=1)\n","\n","    def encode(self, x):\n","        x = self.encoder(x.view(-1, 1, x.shape[1]))\n","        return torch.squeeze(x, dim=1)\n","\n","    def decode(self, x):\n","        x = self.decoder(x)\n","        return torch.squeeze(x, dim=1)\n","\n","    def predict(self, x):\n","        with torch.no_grad():\n","            x_enc = self.encode(x)\n","        x_enc = x_enc.view(-1, x_enc.shape[1]*x_enc.shape[2])\n","        ins = torch.cat([x_enc, x], dim=1)\n","        out = self.fc(ins)\n","        return torch.squeeze(out, dim=1)\n","\n","net = Autoencoder().to(device)\n","summary(net, (len(FEAT_COLS), ))\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":443,"status":"ok","timestamp":1664736911691,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"Elxd377CYRtZ","outputId":"58df6754-b3ba-4970-c6af-9a889d59af71"},"outputs":[{"data":{"text/plain":["Autoencoder(\n","  (encoder): Sequential(\n","    (0): Conv1d(1, 4, kernel_size=(5,), stride=(2,))\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv1d(4, 8, kernel_size=(5,), stride=(2,))\n","    (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): Conv1d(8, 16, kernel_size=(5,), stride=(3,))\n","    (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv1d(16, 32, kernel_size=(5,), stride=(3,))\n","    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (11): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n","    (12): ReLU()\n","  )\n","  (decoder): Sequential(\n","    (0): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(1,))\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(4,))\n","    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): ConvTranspose1d(16, 8, kernel_size=(7,), stride=(4,))\n","    (6): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): ConvTranspose1d(8, 1, kernel_size=(10,), stride=(2,))\n","    (9): Sigmoid()\n","  )\n","  (fc): Sequential(\n","    (0): Linear(in_features=630, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=64, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=64, out_features=1, bias=True)\n","    (5): Sigmoid()\n","  )\n",")"]},"execution_count":14,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["time: 533 ms (started: 2022-10-02 18:55:10 +00:00)\n"]}],"source":["# Load a pytorch model\n","net = Autoencoder().to(device)\n","net.load_state_dict(torch.load(SRCDIR / 'GTRUDA_autoencoder_statedict.pt'))\n","net.eval()"]},{"cell_type":"markdown","metadata":{"id":"oRLGopZF0r7T"},"source":["## Making Predictions with the final model"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89353,"status":"ok","timestamp":1664737001042,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"d9nwKh2C20hT","outputId":"12c62b4f-a30b-4d4d-dacf-2afa34641d58"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2104564, 313)\n","(2104564, 1)\n","(2104564, 314)\n","Scoring predictions on validation data...\n","Spearman:\t0.0215\n","Sharpe:\t\t0.9875\n","Max exposure:\t0.1931\n","RMS exposure:\t0.0816\n","(2104564, 2)\n","time: 1min 29s (started: 2022-10-02 18:55:11 +00:00)\n"]}],"source":["# PyTorch version\n","print(df_tourn.shape)\n","df_predictions = df_tourn.reset_index()[['id']]\n","print(df_predictions.shape)\n","testloader = DataLoader(NumeraiDataset(df_tourn), batch_size=2**12, shuffle=False, num_workers=CPUs)\n","preds = []\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        predicted = list(net.predict(inputs).cpu().numpy())\n","        preds.extend(predicted)\n","df_tourn['prediction'] = preds\n","print(df_tourn.shape)\n","\n","print('Scoring predictions on validation data...')\n","_df = df_tourn[df_tourn.data_type == 'validation']\n","val_corr, val_sharpe, max_fe, rms_fe = scores(_df, verbose=True)\n","\n","# Free up memory\n","# del df_tourn\n","\n","df_predictions['prediction'] = preds\n","print(df_predictions.shape)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1664737001044,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"lqEf66IAoeMw","outputId":"1b979d63-2dfc-463a-dc2d-32a7d0dfe0a6"},"outputs":[{"data":{"text/plain":["count    2104564.000\n","mean           0.499\n","std            0.029\n","min            0.180\n","25%            0.489\n","50%            0.498\n","75%            0.509\n","max            0.792\n","Name: prediction, dtype: float64"]},"execution_count":16,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["time: 85 ms (started: 2022-10-02 18:56:40 +00:00)\n"]}],"source":["df_predictions.prediction.describe().round(3)"]},{"cell_type":"markdown","metadata":{"id":"dRxfYqICVYqv"},"source":["# Submission"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17120,"status":"ok","timestamp":1664737018157,"user":{"displayName":"Gianluca Truda","userId":"10545274143306236899"},"user_tz":-60},"id":"j8UaXQTNVayb","outputId":"06a5bb4c-8995-45c5-d7b6-7189fa6f955c"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 17.1 s (started: 2022-10-02 18:56:40 +00:00)\n"]}],"source":["PRED_FILENAME = f\"{RESULTDIR}/predictions_{MODEL_NAME}_{napi.get_current_round()}.csv\"\n","\n","if SAVE_PREDS:\n","    df_predictions.to_csv(PRED_FILENAME, index=False)\n","\n","if PRED_FILENAME is not None and UPLOAD_PREDS:\n","    napi.upload_predictions(PRED_FILENAME, model_id=MODEL_ID)"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","provenance":[{"file_id":"1ByU1SVGF0m7ae69sz1zD3X5IzKVVv6zk","timestamp":1621764878333},{"file_id":"1J320RCupxF1Inr-QHbrvGadXnBrvbuY4","timestamp":1617398427381},{"file_id":"1ok8nboBobQIdxPzAs13wcybJi5BdkrCe","timestamp":1617379159767},{"file_id":"1ZUM5A6hN8gZXvXk3fq68WP1a690iLniA","timestamp":1617366566515},{"file_id":"1__krUmunJHpDkpfMdDYh7e2S2i5HxgNp","timestamp":1616319326823},{"file_id":"13R96PoB-uGcSbVQfP-mHOUiB-_4scdxv","timestamp":1616099096681},{"file_id":"1ThGqqcA8wc4zuHtolgmAH3m2JrdTZvke","timestamp":1615423425596},{"file_id":"10V6YOGzzgUPBW7trzG95ZicF8cTf_thO","timestamp":1615416690624},{"file_id":"https://github.com/djliden/numerai_starter_kit/blob/main/Numerai_Starter_Kit.ipynb","timestamp":1615411818997}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
